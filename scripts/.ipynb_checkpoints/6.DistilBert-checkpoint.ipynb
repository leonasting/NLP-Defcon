{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5173b71e-e802-43ac-b846-3b595ef9c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering\n",
    "import requests\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e69fb0ee-108c-4f6d-9459-f9d4f9b9aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):  \n",
    "  \n",
    "  with open(path, 'rb') as f:\n",
    "    contract = json.load(f)\n",
    "\n",
    "  contexts = []\n",
    "  questions = []\n",
    "  answers = []\n",
    "\n",
    "  for c in contract:\n",
    "    context = c['context']\n",
    "    for i in range(len(c['questions'])):\n",
    "        question = c[\"questions\"][i]['input_text']\n",
    "        questions.append(question)\n",
    "    for i in range(len(c['answers'])):\n",
    "        answer = c[\"answers\"][i]\n",
    "        contexts.append(context)\n",
    "        answers.append(answer)\n",
    "\n",
    "  return contexts, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0b29056-0c67-4164-b83e-03e07b913963",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contexts, train_questions, train_answers = read_data('def_qa2.json')\n",
    "valid_contexts, valid_questions, valid_answers = read_data('val_qa.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1af26f0c-92ab-4215-ac35-028ca8a9e96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0606cf8fc9cc45e183c9dd7dcab5776c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f05382718b485c91b01c3b84acbf1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d606e10095b743bba755319507b6819a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda78f209ae54c089d28ab4c81c57483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# getting the model and its tokenizer (currently training on only 1000 rows as it is very time consuming)\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "train_encodings = tokenizer(train_contexts[:1000], train_questions[:1000], truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(valid_contexts[:100], valid_questions[:100], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99070c5c-1400-45f5-8d88-ea8f91925510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the answers in the training set for fine tuning\n",
    "def add_token_positions(encodings, answers):\n",
    "  start_positions = []\n",
    "  end_positions = []\n",
    "  for i in range(len(answers)):\n",
    "    start_positions.append(encodings.char_to_token(i, answers[i]['start']))\n",
    "    end_positions.append(encodings.char_to_token(i, answers[i]['end'] - 1))\n",
    "\n",
    "    # if start position is None, the answer passage has been truncated\n",
    "    if start_positions[-1] is None:\n",
    "      start_positions[-1] = tokenizer.model_max_length\n",
    "    if end_positions[-1] is None:\n",
    "      end_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "  encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "add_token_positions(train_encodings, train_answers)\n",
    "add_token_positions(valid_encodings, valid_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1714d3e-f730-49d3-be30-58d7c501d114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the dataset in the format it is required for fine tuning BERT\n",
    "class Def_Dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, encodings):\n",
    "    self.encodings = encodings\n",
    "  def __getitem__(self, idx):\n",
    "    return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "  def __len__(self):\n",
    "    return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5e380a8-50df-4b74-8d7a-851b0109a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Def_Dataset(train_encodings)\n",
    "valid_dataset = Def_Dataset(valid_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3cd288c-563d-4cfb-aa1f-02a673025bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "894dcb17-bd5e-4e87-a924-6bc8fc9277a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03abf99b6cb442394513561f3271fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# loading the BERT model which we will fine tune\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "436d13b7-5b93-4bdd-9bcf-07f96d3cc4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cuda\n"
     ]
    }
   ],
   "source": [
    "# checking the device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'Working on {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc07bd7e-6e62-4c61-9549-9fa7f33b3aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "Epoch 1: 100%|██████████| 5/5 [00:00<00:00,  6.01it/s, loss=5.48]\n",
      "Epoch 2: 100%|██████████| 5/5 [00:00<00:00, 12.88it/s, loss=4.35]\n",
      "Epoch 3: 100%|██████████| 5/5 [00:00<00:00, 12.72it/s, loss=3.68]\n",
      "Epoch 4: 100%|██████████| 5/5 [00:00<00:00, 12.79it/s, loss=3.43]\n",
      "Epoch 5: 100%|██████████| 5/5 [00:00<00:00, 12.80it/s, loss=2.54]\n"
     ]
    }
   ],
   "source": [
    "# Fine tuning it per batch\n",
    "N_EPOCHS = 5\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "  loop = tqdm(train_loader, leave=True)\n",
    "  for batch in loop:\n",
    "    optim.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    start_positions = batch['start_positions'].to(device)\n",
    "    end_positions = batch['end_positions'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "    loss = outputs[0]\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    loop.set_description(f'Epoch {epoch+1}')\n",
    "    loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64f1f56f-d777-4a1e-826d-78d69c82f0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 39.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# checking the performance\n",
    "model.eval()\n",
    "\n",
    "acc = []\n",
    "\n",
    "for batch in tqdm(valid_loader):\n",
    "  with torch.no_grad():\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    start_true = batch['start_positions'].to(device)\n",
    "    end_true = batch['end_positions'].to(device)\n",
    "    \n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "    end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "\n",
    "    acc.append(((start_pred == start_true).sum()/len(start_pred)).item())\n",
    "    acc.append(((end_pred == end_true).sum()/len(end_pred)).item())\n",
    "\n",
    "acc = sum(acc)/len(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f638d7d1-4ab3-48f7-9888-34879b42c907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1428571492433548"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590eb298-43ac-462a-af78-1d14c6e2f226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
