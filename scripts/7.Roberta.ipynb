{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c5500d5-fb80-41fd-90f5-3f41085e237e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.8.1-py3-none-any.whl (10 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2022.10.31-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.1/757.1 kB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.10.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\n",
      "Installing collected packages: tokenizers, regex, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.8.1 huggingface-hub-0.11.1 regex-2022.10.31 tokenizers-0.13.2 transformers-4.25.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dd658e5-4a14-48ee-9d42-79022aee98a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import RobertaTokenizerFast, RobertaForQuestionAnswering\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8d86d22-1e76-48b6-ac60-aed1130aa2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):  \n",
    "  \n",
    "  with open(path, 'rb') as f:\n",
    "    contract = json.load(f)\n",
    "\n",
    "  contexts = []\n",
    "  questions = []\n",
    "  answers = []\n",
    "\n",
    "  for c in contract:\n",
    "    context = c['context']\n",
    "    for i in range(len(c['questions'])):\n",
    "        question = c[\"questions\"][i]['input_text']\n",
    "        questions.append(question)\n",
    "    for i in range(len(c['answers'])):\n",
    "        answer = c[\"answers\"][i]\n",
    "        contexts.append(context)\n",
    "        answers.append(answer)\n",
    "\n",
    "  return contexts, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50c42d6f-49f0-4039-8867-b78ea2d668e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contexts, train_questions, train_answers = read_data('data/def_qa2.json')\n",
    "valid_contexts, valid_questions, valid_answers = read_data('data/val_qa.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a94f2554-cdbc-41be-b2cd-7c51fbb1bef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252251ca51ec4ff09b42459c8fcf7732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89407204c8a94c70ba31411bba3f3b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724febd330204e7a8ffdf14fa7f25fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8daa045d324b21ba9e1bf75c998b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# getting the model and its tokenizer (currently training on only 1000 rows as it is very time consuming)\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-large\")\n",
    "\n",
    "train_encodings = tokenizer(train_contexts[:1000], train_questions[:1000], truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(valid_contexts[:100], valid_questions[:100], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9450f977-7c11-4608-925f-7301f27900b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the answers in the training set for fine tuning\n",
    "def add_token_positions(encodings, answers):\n",
    "  start_positions = []\n",
    "  end_positions = []\n",
    "  for i in range(len(answers)):\n",
    "    start_positions.append(encodings.char_to_token(i, answers[i]['start']))\n",
    "    end_positions.append(encodings.char_to_token(i, answers[i]['end'] - 1))\n",
    "\n",
    "    # if start position is None, the answer passage has been truncated\n",
    "    if start_positions[-1] is None:\n",
    "      start_positions[-1] = tokenizer.model_max_length\n",
    "    if end_positions[-1] is None:\n",
    "      end_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "  encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "add_token_positions(train_encodings, train_answers)\n",
    "add_token_positions(valid_encodings, valid_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b5b32eb-a080-42ea-b0d5-bec03cd714d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the dataset in the format it is required for fine tuning BERT\n",
    "class Def_Dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, encodings):\n",
    "    self.encodings = encodings\n",
    "  def __getitem__(self, idx):\n",
    "    return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "  def __len__(self):\n",
    "    return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98ceaa62-dd3c-4dae-8f31-6dab3caa9aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Def_Dataset(train_encodings)\n",
    "valid_dataset = Def_Dataset(valid_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f3cdcea-3ba7-4576-9928-2da75f209c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fc510d7-05e2-42e6-823b-d43a7dba8695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684120377dba40759bde3a3f5d2040df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-large and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# loading the BERT model which we will fine tune\n",
    "model = RobertaForQuestionAnswering.from_pretrained(\"roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "741c5664-b4a5-4d08-92b7-80047bab87bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Working on cuda\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# checking the device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'Working on {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bc44385-69d5-4a19-a4cc-eeca182c595e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "Epoch 1: 100%|██████████| 5/5 [00:04<00:00,  1.03it/s, loss=4.31]\n",
      "Epoch 2: 100%|██████████| 5/5 [00:02<00:00,  2.05it/s, loss=4.16]\n",
      "Epoch 3: 100%|██████████| 5/5 [00:02<00:00,  2.05it/s, loss=2.82]\n",
      "Epoch 4: 100%|██████████| 5/5 [00:02<00:00,  2.04it/s, loss=2.97]\n",
      "Epoch 5: 100%|██████████| 5/5 [00:02<00:00,  2.04it/s, loss=3.32]\n"
     ]
    }
   ],
   "source": [
    "# Fine tuning it per batch\n",
    "N_EPOCHS = 5\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "  loop = tqdm(train_loader, leave=True)\n",
    "  for batch in loop:\n",
    "    optim.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    start_positions = batch['start_positions'].to(device)\n",
    "    end_positions = batch['end_positions'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "    loss = outputs[0]\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    loop.set_description(f'Epoch {epoch+1}')\n",
    "    loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdf9c945-b86d-4a12-9374-23e46024b72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# checking the performance\n",
    "model.eval()\n",
    "\n",
    "acc = []\n",
    "\n",
    "for batch in tqdm(valid_loader):\n",
    "  with torch.no_grad():\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    start_true = batch['start_positions'].to(device)\n",
    "    end_true = batch['end_positions'].to(device)\n",
    "    \n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "    end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "\n",
    "    acc.append(((start_pred == start_true).sum()/len(start_pred)).item())\n",
    "    acc.append(((end_pred == end_true).sum()/len(end_pred)).item())\n",
    "\n",
    "acc = sum(acc)/len(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a52c86d4-cd1c-4207-8aa7-ddbb5c6f48e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1428571492433548"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8361ea-27ec-432d-809f-bee8da8c908e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m100"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
