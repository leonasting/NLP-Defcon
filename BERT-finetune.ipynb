{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e692141f-daa9-4871-877c-11946f02cfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import requests\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eccf232-0f74-429c-81d0-9da33a29c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dev_qa2.json', 'rb') as f:\n",
    "  def_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c57727f-cbe8-4129-a290-3f80f4546255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):  \n",
    "  \n",
    "  with open(path, 'rb') as f:\n",
    "    def_dataset = json.load(f)\n",
    "  \n",
    "  contexts = []\n",
    "  questions = []\n",
    "  answers = []\n",
    "\n",
    "  for contract in def_dataset:\n",
    "    context = contract['context']\n",
    "    for qa in contract['questions']:\n",
    "        question = qa['input_text']\n",
    "        questions.append(question)\n",
    "    for answer in contract['answers']:\n",
    "        contexts.append(context)\n",
    "        answers.append(answer)\n",
    "\n",
    "  return contexts, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d21ed49-0a57-40f6-9f3a-fcc6d58962dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contexts, train_questions, train_answers = read_data('dev_qa2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57ba7c0a-de1f-44b5-ac25-f72c74538c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the model and its tokenizer (currently training on only 1000 rows as it is very time consuming)\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(train_contexts[:1000], train_questions[:1000], truncation=True, padding=True)\n",
    "#valid_encodings = tokenizer(valid_contexts[:100], valid_questions[:100], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b916a3a-a923-4f17-ad94-4d59164b433f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
